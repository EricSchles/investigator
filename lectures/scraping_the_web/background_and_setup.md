#Background and setup - I'm going to 'install' some ideas in you.

Scraping the web for data is about mapping one representation into another, often a database.  With this transformation of information into a local context, certain information maybe lost, or abstracted.  Thusly, the idea of scraping the web can be hard, but doing the actual scraping is usually straight forward.  

The important thing to understand is not the mapping of context from one technical medium - the web, to another - a database.  But instead the ideological mapping, from a dynamic context to a static one.  For this we need metadata about our web information and semantic information about the domain we are scraping.  This will allow us the ability to compress unnecessary information, infer missing data from backend processes we can't see, and make sense of the unstructured semantics of the web.

What this translates to?  Taking information from one database and sending it to another, via a Rube Goldberg machine of tools and techniques.
