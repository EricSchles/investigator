#Bringing ideas and technology together

Now that we've established the conceptual framework for what we are going to measure, we are in a place to begin attacking the problem.  For this, I'll be showing you how to scrape backpage.  It's worth noting website like backpage are adversial in nature, they don't want you to scrape their data.  So extra challenges will occur that will lead us down some technically challenging paths.

##Plan

We'll build up the terms and tools methodically for scaping.

* Pick a place to scrape on backpage - backpage is ordered by city and state and there are backpage websites for each location (much like lots of the federal government).  

* Pick a subset of the adult content to scrape

* Start scraping

* decide what to store and how to store it

* set up our database

* do exploratory data analysis around our data

* create data visualizations of our findings

* do prediction and model selection to understand our data in detail

* do probabilistic analysis to understand our underlying distributions and how different random variables inter relate

* generalize out our scraper to work over all of backpage

* start scaling out our data concerns to allow 

* bring in other data sources
