###Our first metric

Understanding how to relate craigslist and backpage in this context is our first web scraping task.  We can use craigslist to get a sense of the full market size for a given community.  Where a full community is modeled as a craigslist for an area.  And then we can treat the market segment interested in the purchase and sale of sex as those represented on backpage.  Thus we can get an accurate sense of market size and a metric I will call relative willingness to purchase sex commercially.  It's important to note - not all commercial sex is sex slavery, but all sex slavery is commercial sex.  So how do we get to a precise answer of how much is going on?  Using only data from the web, we don't.  Instead, we will write our tools from the context of an investigator working in a district attorney's office or police department.  Thus, we never know how bad or terrible the sex trafficking happening is, with precision.  However, we will still have an upper bound and isn't that all computer scientists care about anyway ;)  For clarity, the joke here is comparing measuring the size of sex trafficking to big Oh-notation, I am not claiming computer scientists don't care.  (They care a lot!)

Investigators have the opposite problem, as compared to the civicly minded developer - all the data and none of the resources.  By creating tools for such an investigator, we can implicitly aid in the search for human traffickers.  But more over, by understanding how to build for investigators, we don't run the legal, or personal risk of investigating dangerous criminals.  

So to recap - our first metric comes from taking the proportion of web traffic from craigslist and backpage's adult section and comparing the two.  The resulting proportion will tell us the market segment interested in purchasing sex.  And we will define it as relative willingness to purchase commercial sex.
