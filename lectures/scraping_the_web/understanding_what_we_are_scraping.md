#Understanding What We Are Scraping

Now that we have some notion of how to scrape data from the internet, I'm going to bring in some background into what we'll be scraping.  In this case we'll be scraping backpage - looking instances of human trafficking.  Understanding that the data is out there, is not enough to bring people to justice for the things they've done, but it's a start.

The truth is we are investigating a set of complex actors that work, often times together in complex ways.  Capturing this complexity can be a challenge.  But it's best to start with the market place.  By understanding how to get information from backpage, we can abstract out to the pocket websites which capture information about sex slavery.  To put a more economic context to our understanding - backpage can be thought of as the supply side of sex slavery whereas, understanding the buyers, informs the demand side.  At present little formal work has been to understand these market places.  People tend to focus on specific cases or investigations, rather than understanding the markets as a whole.  

The ultimate goal of what we are trying to do here is complex - to understand the forces that contribute to changes in the supply and demand of sex slavery.  And if possible, to manipulate the market into contracting.  To do this, we need a set of effective data driven policy levers.  By understanding the scraped data, we have insight into just that.  At the very least, by fully understanding the market place for sex slavery, we force the actors to "go underground" to the deep web.  While some people will follow, it is possible many of the less technically sophisticated actors won't.  In which case, just by inspecting the market, we shrink it.  The best case scenario, is no deep web retreat happens, and traffickers continue to act in the open.  In this case, we can leverage their data to make real policy decisions, to systematically shrink the market, in a dynamic, reactive way.  
